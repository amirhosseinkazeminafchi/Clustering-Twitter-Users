{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from twitter import *\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-----------------------------------------------------------------------\n",
    "# load our API credentials  from the config.py file\n",
    "#-----------------------------------------------------------------------\n",
    "config = {}\n",
    "exec(open('config.py').read(), config)\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "# create twitter API object\n",
    "#-----------------------------------------------------------------------\n",
    "twitter = Twitter(auth = OAuth(config[\"access_key\"], config[\"access_secret\"], config[\"consumer_key\"], config[\"consumer_secret\"]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to parse twitter JSON\n",
    "#json.loads : Parse JSON - Convert from JSON to Python\n",
    "#json.dumps : Convert from Python to JSON\n",
    "\n",
    "def pp_json(json_thing, sort=True, indents=4):\n",
    "    if type(json_thing) is str:\n",
    "        print(json.dumps(json.loads(json_thing), sort_keys=sort, indent=indents))\n",
    "    else:\n",
    "        print(json.dumps(json_thing, sort_keys=sort, indent=indents))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data folder if it does not exists\n",
    "if not 'data' in os.listdir():\n",
    "    os.mkdir('data')\n",
    "    \n",
    "#####################################################################\n",
    "### MAKE EMPTY DF To POPULATE WITH DATA ###########\n",
    "\n",
    "df = pd.DataFrame(columns = ['created_at', 'tweet_id', 'text', 'url', 'retweet_count', 'status'])\n",
    "df.to_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, set the earliest date of tweets to be grabbed.  So that all posts of that particular day is grabbed. However,  twitter api only allows around 3200 posts to be collected for one particular account. Hence, loop is designed to break once that number is reached which, depending on your settings, could happen before the earliest date is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_date = 'Jan 17' #Set it to one day before the date you want.\n",
    "screen_name = 'nytimes' #twitter screen name \n",
    "file_path = 'data/nytimesposts.csv' #file path for the csv file that will store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-----------------------------------------------------------------------\n",
    "# loop through each of newsoutlet statuses, and print its content\n",
    "#-----------------------------------------------------------------------\n",
    "count = 0\n",
    "\n",
    "posts_collected = 0\n",
    "\n",
    "# this is for breaking the loop if/when earliest_data is reached. \n",
    "# _continue is set to false when earliest_date is reached on line 58\n",
    "_continue = True \n",
    "\n",
    "# every request gets you 200 tweets. So for each consecutive requests after the first one, we need to track \n",
    "# the id of the last tweet grabbed. So the next request grabs tweets before the last id.  \n",
    "\n",
    "last_id = 0\n",
    "\n",
    "while True:\n",
    " # Make API CALL\n",
    "    try:\n",
    "        \n",
    "        if last_id == 0:\n",
    "            count +=1\n",
    "            print('request', count)\n",
    "            data = twitter.statuses.user_timeline(screen_name=screen_name, count=200)\n",
    "        else:\n",
    "            # checking if the ealiest date has been reached or not.\n",
    "            if _continue == False:\n",
    "                print (f'DONE! All posts collected till {earliest_date}, 2019')\n",
    "                break\n",
    "\n",
    "            count +=1\n",
    "            print('request', count)\n",
    "            print('lastid', last_id)\n",
    "            data = twitter.statuses.user_timeline(screen_name=screen_name, max_id = last_id-1, count=200)\n",
    "\n",
    "        if len(data) == 0:\n",
    "            #twitter API only allows around 3200 tweets to be collected for one particular user. \n",
    "            # After maxing out it returns an empty list.\n",
    "            print (f'Maximum number of posts for one user reached, total no. of posts collected: {posts_collected}')\n",
    "            break\n",
    "\n",
    "        last_id = data[-1]['id']\n",
    "\n",
    "    except Exception as E:\n",
    "        print(E)\n",
    "        # Max requests per 15 minutes is 75. The following will print out the message from twitter server \n",
    "        pp_json(twitter.application.rate_limit_status()[\"resources\"][\"statuses\"][\"/statuses/user_timeline\"])\n",
    "        print (\"sleeping...\")\n",
    "        time.sleep(60*15)\n",
    "\n",
    "    #Check how many posts grabbed\n",
    "    posts_collected += len(data)\n",
    "    print('Total posts collected:', posts_collected)\n",
    "\n",
    "    # Loop through the posts to get information and populate df\n",
    "    for status in data:\n",
    "        if earliest_date in status['created_at']:\n",
    "            _continue = False\n",
    "            break\n",
    "\n",
    "        mini_df = pd.DataFrame()\n",
    "        mini_df['created_at'] = [status['created_at']]\n",
    "        mini_df['tweet_id'] = [status['id']]\n",
    "        mini_df['text'] = [status['text']]\n",
    "        mini_df['url'] =  [status['user']['entities']['description']['urls'][0]['expanded_url']]\n",
    "        mini_df['retweet_count'] = [status['retweet_count']]\n",
    "        mini_df['status'] = [json.dumps(status)]\n",
    "        mini_df.to_csv(file_path, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Retweeter List \n",
    "\n",
    "To collect list of retweeter id for each tweets collected above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df= pd.read_csv('data/nytimesposts.csv')\n",
    "\n",
    "\n",
    "### MAKE EMPTY DF To POPULATE WITH DATA \n",
    "file_path = 'data/retweeterlist.csv'\n",
    "new_path = 'data/nytimesposts1.csv'\n",
    "\n",
    "df1 = pd.DataFrame(columns = ['tweet_id', 'retweeter_list'])\n",
    "df1.to_csv(file_path)\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    \n",
    "    try:\n",
    "        mini_df = pd.DataFrame()\n",
    "        r = twitter.statuses.retweeters.ids( _id=df.iloc[i, 2])['ids']\n",
    "        mini_df['tweet_id'] = [df.iloc[i, 2]]\n",
    "        mini_df['retweeter_list'] = [r]\n",
    "        mini_df.to_csv(file_path, mode='a', header=False)\n",
    "        print(f'index:{i}',  f'retweeter count:{len(r)}', 'original retweeter count:', df.iloc[i, 5])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print (\"sleeping...\")\n",
    "        pp_json(twitter.application.rate_limit_status()[\"resources\"][\"statuses\"][\"/statuses/retweeters/ids\"])\n",
    "        time.sleep(60*15)\n",
    "\n",
    "df1 = pd.read_csv(file_path)\n",
    "\n",
    "if df.shape[0] == df1.shape[0]:\n",
    "    df['retweeter_list'] = df1['retweeter_list']\n",
    "    df.to_csv(new_path)\n",
    "else:\n",
    "    print('length of df and df1 do not match')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole process for entier 3200 is over 14 hours. So, to save time while this runs  I also run the get_user_info.py file in terminal to simultaneously get the individual user information as the retweeterlist.csv udpates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
